import { motion } from 'framer-motion'
import { InlineMath, BlockMath } from 'react-katex'
import 'katex/dist/katex.min.css'

export default function PolicyGradientLesson() {
  return (
    <div className="max-w-5xl mx-auto space-y-8">
      {/* Introduction */}
      <section className="card">
        <h2 className="text-2xl font-bold mb-4">Policy Gradient Methods</h2>
        <p className="text-gray-700 mb-4">
          Unlike value-based methods (Q-Learning), policy gradient methods directly optimize 
          the policy <InlineMath math="\pi_\theta(a|s)" /> by following the gradient of expected return.
        </p>
        
        <div className="bg-purple-50 border-l-4 border-purple-500 p-4 my-4">
          <p className="font-semibold text-purple-900">Key Difference:</p>
          <p className="text-purple-800">
            Instead of learning Q-values and deriving a policy, we parameterize the policy directly 
            and optimize it with gradient ascent.
          </p>
        </div>
      </section>
      
      {/* From Value to Policy */}
      <section className="card">
        <h3 className="text-xl font-bold mb-4">From Value-Based to Policy-Based</h3>
        
        <div className="grid md:grid-cols-2 gap-6">
          <div className="border-2 border-blue-300 rounded-lg p-4">
            <h4 className="font-semibold text-blue-900 mb-2">Value-Based (Q-Learning)</h4>
            <ul className="text-sm space-y-2">
              <li>‚úì Learn Q(s,a)</li>
              <li>‚úì Policy derived: <InlineMath math="\pi(s) = \arg\max_a Q(s,a)" /></li>
              <li>‚úì Deterministic policy</li>
              <li>‚úì Works for discrete actions</li>
              <li>‚úó Hard for continuous actions</li>
            </ul>
          </div>
          
          <div className="border-2 border-purple-300 rounded-lg p-4">
            <h4 className="font-semibold text-purple-900 mb-2">Policy-Based (REINFORCE, PPO)</h4>
            <ul className="text-sm space-y-2">
              <li>‚úì Learn <InlineMath math="\pi_\theta(a|s)" /> directly</li>
              <li>‚úì Can be stochastic</li>
              <li>‚úì Works for continuous actions</li>
              <li>‚úì Better convergence properties</li>
              <li>‚úó High variance (but fixable)</li>
            </ul>
          </div>
        </div>
      </section>
      
      {/* REINFORCE Algorithm */}
      <section className="card">
        <h3 className="text-xl font-bold mb-4">REINFORCE: Monte Carlo Policy Gradient</h3>
        
        <div className="space-y-6">
          <div>
            <h4 className="font-semibold mb-2">Objective Function:</h4>
            <BlockMath math="J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]" />
            <p className="text-sm text-gray-600 mt-2">
              Maximize expected return over trajectories <InlineMath math="\tau" /> generated by policy <InlineMath math="\pi_\theta" />.
            </p>
          </div>
          
          <div>
            <h4 className="font-semibold mb-2">Policy Gradient Theorem:</h4>
            <BlockMath math="\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t\right]" />
            <p className="text-sm text-gray-600 mt-2">
              The gradient points in the direction that increases the probability of good actions (high return <InlineMath math="G_t" />).
            </p>
          </div>
          
          <div>
            <h4 className="font-semibold mb-2">Update Rule:</h4>
            <BlockMath math="\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t" />
            <p className="text-sm text-gray-600 mt-2">
              Increase probability of actions that led to high returns, decrease probability of actions that led to low returns.
            </p>
          </div>
        </div>
      </section>
      
      {/* PPO Algorithm */}
      <section className="card bg-gradient-to-br from-purple-50 to-pink-50">
        <h3 className="text-xl font-bold mb-4">üöÄ PPO: Proximal Policy Optimization</h3>
        <p className="text-gray-700 mb-4">
          PPO is the state-of-the-art policy gradient method, used by OpenAI for ChatGPT's RLHF training 
          and many other applications. It improves upon REINFORCE by constraining policy updates.
        </p>
        
        <div className="space-y-6">
          <div>
            <h4 className="font-semibold mb-2">The Problem with Large Updates:</h4>
            <p className="text-sm text-gray-600">
              In vanilla policy gradient, large parameter updates can drastically change the policy, 
              leading to instability and poor performance.
            </p>
          </div>
          
          <div>
            <h4 className="font-semibold mb-2">PPO's Solution - Clipped Surrogate Objective:</h4>
            <BlockMath math="L^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_t\right)\right]" />
            
            <div className="mt-4 space-y-2 text-sm">
              <p>Where:</p>
              <ul className="list-disc list-inside ml-4">
                <li><InlineMath math="r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}" /> (probability ratio)</li>
                <li><InlineMath math="\hat{A}_t" /> is the advantage estimate (how much better than average)</li>
                <li><InlineMath math="\varepsilon" /> is the clipping parameter (typically 0.2)</li>
              </ul>
            </div>
          </div>
          
          <div className="bg-white p-4 rounded-lg">
            <h4 className="font-semibold mb-2">What Does Clipping Do?</h4>
            <p className="text-sm text-gray-700">
              The <InlineMath math="\text{clip}" /> function prevents the new policy from being too different 
              from the old policy. If <InlineMath math="r_t(\theta)" /> goes outside <InlineMath math="[1-\varepsilon, 1+\varepsilon]" />, 
              we ignore further improvement in that direction.
            </p>
          </div>
        </div>
      </section>
      
      {/* Actor-Critic */}
      <section className="card">
        <h3 className="text-xl font-bold mb-4">Actor-Critic Architecture</h3>
        <p className="text-gray-700 mb-4">
          Modern policy gradient methods (including PPO) use an <strong>Actor-Critic</strong> architecture:
        </p>
        
        <div className="grid md:grid-cols-2 gap-6">
          <div className="bg-blue-50 p-4 rounded-lg">
            <h4 className="font-semibold text-blue-900 mb-2">üé≠ Actor (Policy Network)</h4>
            <p className="text-sm text-gray-700">
              Outputs action probabilities: <InlineMath math="\pi_\theta(a|s)" />
            </p>
            <p className="text-sm text-gray-600 mt-2">
              "What action should I take?"
            </p>
          </div>
          
          <div className="bg-green-50 p-4 rounded-lg">
            <h4 className="font-semibold text-green-900 mb-2">üé¨ Critic (Value Network)</h4>
            <p className="text-sm text-gray-700">
              Estimates state value: <InlineMath math="V_\phi(s)" />
            </p>
            <p className="text-sm text-gray-600 mt-2">
              "How good is this state?"
            </p>
          </div>
        </div>
        
        <div className="mt-4">
          <h4 className="font-semibold mb-2">Advantage Function:</h4>
          <BlockMath math="A(s,a) = Q(s,a) - V(s)" />
          <p className="text-sm text-gray-600">
            Tells us how much better action <InlineMath math="a" /> is compared to the average action in state <InlineMath math="s" />.
          </p>
        </div>
      </section>
      
      {/* Comparison Table */}
      <section className="card">
        <h3 className="text-xl font-bold mb-4">Algorithm Comparison</h3>
        
        <div className="overflow-x-auto">
          <table className="w-full text-sm">
            <thead>
              <tr className="border-b-2 border-gray-300">
                <th className="text-left p-2">Algorithm</th>
                <th className="text-left p-2">Type</th>
                <th className="text-left p-2">On/Off Policy</th>
                <th className="text-left p-2">Continuous Actions</th>
                <th className="text-left p-2">Sample Efficiency</th>
              </tr>
            </thead>
            <tbody>
              <tr className="border-b border-gray-200">
                <td className="p-2 font-medium">Q-Learning</td>
                <td className="p-2">Value-based</td>
                <td className="p-2 text-green-600">Off-policy</td>
                <td className="p-2 text-red-600">‚úó</td>
                <td className="p-2 text-green-600">High</td>
              </tr>
              <tr className="border-b border-gray-200">
                <td className="p-2 font-medium">REINFORCE</td>
                <td className="p-2">Policy gradient</td>
                <td className="p-2 text-yellow-600">On-policy</td>
                <td className="p-2 text-green-600">‚úì</td>
                <td className="p-2 text-red-600">Low</td>
              </tr>
              <tr className="border-b border-gray-200">
                <td className="p-2 font-medium">TRPO</td>
                <td className="p-2">Policy gradient</td>
                <td className="p-2 text-yellow-600">On-policy</td>
                <td className="p-2 text-green-600">‚úì</td>
                <td className="p-2 text-yellow-600">Medium</td>
              </tr>
              <tr className="border-b border-gray-200">
                <td className="p-2 font-medium">PPO</td>
                <td className="p-2">Policy gradient</td>
                <td className="p-2 text-yellow-600">On-policy</td>
                <td className="p-2 text-green-600">‚úì</td>
                <td className="p-2 text-yellow-600">Medium</td>
              </tr>
              <tr>
                <td className="p-2 font-medium">DQN</td>
                <td className="p-2">Value-based (NN)</td>
                <td className="p-2 text-green-600">Off-policy</td>
                <td className="p-2 text-red-600">‚úó</td>
                <td className="p-2 text-green-600">High</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      
      {/* TODO Placeholder */}
      <section className="card bg-yellow-50 border-2 border-yellow-300">
        <h3 className="text-xl font-bold mb-4">üöß Interactive PPO Simulation (Coming Soon)</h3>
        <p className="text-gray-700 mb-4">
          The full PPO implementation with neural networks is complex and computationally intensive. 
          For now, check out the <strong>Playground</strong> for a simplified policy gradient simulation.
        </p>
        
        <div className="space-y-2 text-sm text-gray-700">
          <p><strong>TODO:</strong> Implement full PPO with:</p>
          <ul className="list-disc list-inside ml-4">
            <li>Actor and Critic neural networks</li>
            <li>Generalized Advantage Estimation (GAE)</li>
            <li>Mini-batch training</li>
            <li>Real-time visualization of policy evolution</li>
          </ul>
        </div>
      </section>
      
      {/* Real-World Applications */}
      <section className="card bg-gradient-to-br from-blue-50 to-purple-50">
        <h3 className="text-xl font-bold mb-4">üåç Real-World Applications of PPO</h3>
        <ul className="space-y-3">
          <li className="flex items-start gap-3">
            <span className="text-2xl">ü§ñ</span>
            <div>
              <strong>Robotics:</strong> Training robots for manipulation, locomotion, and navigation
            </div>
          </li>
          <li className="flex items-start gap-3">
            <span className="text-2xl">üéÆ</span>
            <div>
              <strong>Game AI:</strong> Dota 2 (OpenAI Five), Atari games, continuous control tasks
            </div>
          </li>
          <li className="flex items-start gap-3">
            <span className="text-2xl">üí¨</span>
            <div>
              <strong>Language Models:</strong> RLHF (Reinforcement Learning from Human Feedback) for ChatGPT, GPT-4
            </div>
          </li>
          <li className="flex items-start gap-3">
            <span className="text-2xl">üöó</span>
            <div>
              <strong>Autonomous Driving:</strong> Path planning and decision making
            </div>
          </li>
        </ul>
      </section>
      
      {/* Key Insights */}
      <section className="card bg-gradient-to-br from-green-50 to-blue-50">
        <h3 className="text-xl font-bold mb-3">üí° Key Insights</h3>
        <ul className="space-y-2 text-gray-700">
          <li className="flex items-start gap-2">
            <span className="text-green-600 font-bold">‚Ä¢</span>
            <span>Policy gradient methods directly optimize what we care about (the policy)</span>
          </li>
          <li className="flex items-start gap-2">
            <span className="text-green-600 font-bold">‚Ä¢</span>
            <span>PPO is currently the most popular deep RL algorithm due to its simplicity and stability</span>
          </li>
          <li className="flex items-start gap-2">
            <span className="text-green-600 font-bold">‚Ä¢</span>
            <span>Actor-Critic combines value-based and policy-based methods for lower variance</span>
          </li>
          <li className="flex items-start gap-2">
            <span className="text-green-600 font-bold">‚Ä¢</span>
            <span>Clipping in PPO prevents destructive policy updates</span>
          </li>
          <li className="flex items-start gap-2">
            <span className="text-green-600 font-bold">‚Ä¢</span>
            <span>For continuous control and complex tasks, policy gradients often outperform value-based methods</span>
          </li>
        </ul>
      </section>
    </div>
  )
}

